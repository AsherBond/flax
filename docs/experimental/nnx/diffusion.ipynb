{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets.load import load_dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "  batch_size: int = 32\n",
    "  img_size: int = 128\n",
    "  epochs: int = 500\n",
    "  total_samples: int = 5_000_000\n",
    "  loss_type: str = 'mae'\n",
    "  dataset: str = 'cartoonset'\n",
    "  viz: str = 'matplotlib'\n",
    "  model: str = 'stable_unet'\n",
    "  eval_every: int = 200\n",
    "  log_every: int = 200\n",
    "  # model config\n",
    "  channels_in: int = 3\n",
    "  channels_block: int = 128\n",
    "  channels_mlp: int = 128\n",
    "  patches_mlp: int = 128\n",
    "  num_layers: int = 8\n",
    "  patch_size: int = 8\n",
    "  num_patch_rows: int = 16\n",
    "  # ema config\n",
    "  ema_decay: float = 0.995\n",
    "  ema_update_every: int = 10\n",
    "  ema_update_after_step: int = 100\n",
    "  # schedule config\n",
    "  schedule: str = 'cosine'\n",
    "  beta_start: float = 3e-4\n",
    "  beta_end: float = 0.5\n",
    "  timesteps: int = 1_000\n",
    "  # optimizer config\n",
    "  lr_start: float = 2e-5\n",
    "  drop_1_mult: float = 1.0\n",
    "  drop_2_mult: float = 1.0\n",
    "\n",
    "  @property\n",
    "  def steps_per_epoch(self) -> int:\n",
    "    return self.total_samples // (self.epochs * self.batch_size)\n",
    "\n",
    "  @property\n",
    "  def total_steps(self) -> int:\n",
    "    return self.total_samples // self.batch_size\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.dataset == 'mnist':\n",
    "  hfds = load_dataset('mnist', split='train')\n",
    "  X = np.stack(hfds['image'])[..., None]\n",
    "  ds = tf.data.Dataset.from_tensor_slices(X.astype(np.float32))\n",
    "elif config.dataset == 'pokemon':\n",
    "  hfds = load_dataset('lambdalabs/pokemon-blip-captions', split='train')\n",
    "  hfds = hfds.map(\n",
    "    lambda sample: {'image': sample['image'].resize((64 + 16, 64 + 16))},\n",
    "    remove_columns=['text'],\n",
    "    batch_size=96,\n",
    "  )\n",
    "  X = np.stack(hfds['image'])\n",
    "  ds = tf.data.Dataset.from_tensor_slices(X.astype(np.float32))\n",
    "elif config.dataset == 'cartoonset':\n",
    "  hfds = load_dataset('cgarciae/cartoonset', '10k', split='train')\n",
    "  ds = tf.data.Dataset.from_generator(\n",
    "    lambda: hfds,\n",
    "    output_signature={\n",
    "      'img_bytes': tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "    },\n",
    "  )\n",
    "\n",
    "  def process_fn(x):\n",
    "    x = tf.image.decode_png(x['img_bytes'], channels=3)\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = tf.image.resize(x, (128, 128))\n",
    "    return x\n",
    "\n",
    "  ds = ds.map(process_fn)\n",
    "else:\n",
    "  raise ValueError(f'Unknown dataset {config.dataset}')\n",
    "\n",
    "ds = ds.map(lambda x: x / 127.5 - 1.0)\n",
    "ds = ds.repeat()\n",
    "ds = ds.shuffle(seed=42, buffer_size=1_000)\n",
    "ds = ds.batch(config.batch_size, drop_remainder=True)\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einop import einop\n",
    "\n",
    "x_sample: np.ndarray = ds.as_numpy_iterator().next()\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 7\n",
    "t = x_sample[: n_rows * n_cols]\n",
    "plt.figure(figsize=(3 * n_cols, 3 * n_rows))\n",
    "t = einop(t, '(row col) h w c -> (row h) (col w) c', row=n_rows, col=n_cols)\n",
    "plt.imshow(t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.experimental import nnx\n",
    "\n",
    "\n",
    "def expand_to(a: jax.Array, b: jax.Array):\n",
    "  new_shape = a.shape + (1,) * (b.ndim - a.ndim)\n",
    "  return a.reshape(new_shape)\n",
    "\n",
    "\n",
    "class ScheduleState(nnx.Variable):\n",
    "  pass\n",
    "\n",
    "\n",
    "class DDPMSchedule(nnx.GraphNode):\n",
    "  def __init__(self, betas: jax.Array, timesteps: int, rngs: nnx.Rngs):\n",
    "    self.timesteps = timesteps\n",
    "    self.betas = ScheduleState(betas)\n",
    "    self.alphas = ScheduleState(1.0 - betas)\n",
    "    self.alpha_bars = ScheduleState(jnp.cumprod(1.0 - betas))\n",
    "    self.rngs = rngs\n",
    "\n",
    "  @nnx.jit\n",
    "  def forward(self, x: jax.Array, t: jax.Array):\n",
    "    key = self.rngs.schedule()\n",
    "    alpha_bars = expand_to(self.alpha_bars[t], x)\n",
    "    noise = jax.random.normal(key, x.shape)\n",
    "    xt = jnp.sqrt(alpha_bars) * x + jnp.sqrt(1.0 - alpha_bars) * noise\n",
    "    return xt, noise\n",
    "\n",
    "  @nnx.jit\n",
    "  def reverse(self, x: jax.Array, noise: jax.Array, t: jax.Array):\n",
    "    betas = expand_to(self.betas[t], x)\n",
    "    alphas = expand_to(self.alphas[t], x)\n",
    "    alpha_bars = expand_to(self.alpha_bars[t], x)\n",
    "\n",
    "    key = self.rngs.schedule()\n",
    "    z = jnp.where(\n",
    "      expand_to(t, x) > 0, jax.random.normal(key, x.shape), jnp.zeros_like(x)\n",
    "    )\n",
    "    a = 1.0 / jnp.sqrt(alphas)\n",
    "    b = betas / jnp.sqrt(1.0 - alpha_bars)\n",
    "    x = a * (x - b * noise) + jnp.sqrt(betas) * z\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_schedule(\n",
    "  beta_start, beta_end, timesteps, exponent=2.0, **kwargs\n",
    "):\n",
    "  betas = jnp.linspace(0, 1, timesteps) ** exponent\n",
    "  return betas * (beta_end - beta_start) + beta_start\n",
    "\n",
    "\n",
    "def sigmoid_schedule(beta_start, beta_end, timesteps, **kwargs):\n",
    "  betas = jax.nn.sigmoid(jnp.linspace(-6, 6, timesteps))\n",
    "  return betas * (beta_end - beta_start) + beta_start\n",
    "\n",
    "\n",
    "def cosine_schedule(beta_start, beta_end, timesteps, s=0.008, **kwargs):\n",
    "  x = jnp.linspace(0, timesteps, timesteps + 1)\n",
    "  ft = jnp.cos(((x / timesteps) + s) / (1 + s) * jnp.pi * 0.5) ** 2\n",
    "  alphas_cumprod = ft / ft[0]\n",
    "  betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "  betas = jnp.clip(betas, 0.0001, 0.9999)\n",
    "  betas = (betas - betas.min()) / (betas.max() - betas.min())\n",
    "  return betas * (beta_end - beta_start) + beta_start\n",
    "\n",
    "\n",
    "# %%\n",
    "if config.schedule == 'polynomial':\n",
    "  schedule_fn = polynomial_schedule\n",
    "elif config.schedule == 'sigmoid':\n",
    "  schedule_fn = sigmoid_schedule\n",
    "elif config.schedule == 'cosine':\n",
    "  schedule_fn = cosine_schedule\n",
    "else:\n",
    "  raise ValueError(f'Unknown schedule {config.schedule}')\n",
    "\n",
    "betas = schedule_fn(\n",
    "  config.beta_start,\n",
    "  config.beta_end,\n",
    "  config.timesteps,\n",
    ")\n",
    "rngs = nnx.Rngs(0)\n",
    "schedule = DDPMSchedule(betas, config.timesteps, rngs)\n",
    "n_rows = 2\n",
    "n_cols = 7\n",
    "\n",
    "_, (ax_img, ax_plot) = plt.subplots(2, 1, figsize=(3 * n_cols, 3 * n_rows))\n",
    "\n",
    "t = jnp.linspace(0, config.timesteps, n_cols).astype(jnp.uint32)\n",
    "x_data = einop(x_sample[0], 'h w c -> b h w c', b=n_cols)\n",
    "x_data, x_data_noise = schedule.forward(x_data, t)\n",
    "x_viz = einop(x_data, 'col h w c -> h (col w) c', col=n_cols)\n",
    "x_viz = (x_viz + 1.0) / 2.0\n",
    "ax_img.imshow(x_viz)\n",
    "\n",
    "linear = polynomial_schedule(\n",
    "  betas.min(), betas.max(), config.timesteps, exponent=1.0\n",
    ")\n",
    "ax_plot.plot(linear, label='linear', color='black', linestyle='dotted')\n",
    "ax_plot.plot(betas)\n",
    "for s in ['top', 'bottom', 'left', 'right']:\n",
    "  ax_plot.spines[s].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(nnx.jit, static_argnames=['return_all'])\n",
    "def sample(\n",
    "  model: nnx.Module,\n",
    "  x0: jax.Array,\n",
    "  ts: jax.Array,\n",
    "  schedule: DDPMSchedule,\n",
    "  *,\n",
    "  return_all=True,\n",
    "):\n",
    "  ts = einop(ts, 't -> t b', b=x0.shape[0])\n",
    "\n",
    "  @partial(nnx.scan, state_axes={}, split_rngs=True)\n",
    "  def scan_fn(\n",
    "    x: jax.Array, t: jax.Array, model: nnx.Module, schedule: DDPMSchedule\n",
    "  ):\n",
    "    pred_noise = model(x, t)\n",
    "    x = schedule.reverse(x, pred_noise, t)\n",
    "    if return_all:\n",
    "      return x, x\n",
    "    else:\n",
    "      return x, None\n",
    "\n",
    "  x, xs = scan_fn(x0, ts, model, schedule)\n",
    "\n",
    "  if xs is not None:\n",
    "    return xs\n",
    "  else:\n",
    "    return x\n",
    "\n",
    "\n",
    "def render_image(x, ax=None):\n",
    "  if ax is None:\n",
    "    ax = plt.gca()\n",
    "\n",
    "  if x.shape[-1] == 1:\n",
    "    x = x[..., 0]\n",
    "    cmap = 'gray'\n",
    "  else:\n",
    "    cmap = None\n",
    "  x = (x / 2.0 + 0.5) * 255\n",
    "  x = np.clip(x, 0, 255).astype(np.uint8)\n",
    "  ax.imshow(x, cmap=cmap)\n",
    "  ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nnx.Module):\n",
    "  def __init__(self, sample: jax.Array):\n",
    "    self.sample = sample\n",
    "\n",
    "  def __call__(self, x, t):\n",
    "    return 10.0 * (x - self.sample[None])\n",
    "\n",
    "\n",
    "baseline = Baseline(x_sample[0])\n",
    "\n",
    "x0 = jax.random.normal(rngs.sample(), x_sample.shape)\n",
    "ts = jnp.arange(config.timesteps)[::-1]\n",
    "xf = sample(baseline, x0, ts, schedule, return_all=False)\n",
    "\n",
    "nnx.display(xf[0])\n",
    "render_image(xf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from flax.experimental import nnx\n",
    "\n",
    "\n",
    "def MLP(din, dmid, dout, rngs):\n",
    "  return nnx.Sequential(\n",
    "    nnx.Linear(din, dmid, rngs=rngs),\n",
    "    nnx.relu,\n",
    "    nnx.Linear(dmid, dout, rngs=rngs),\n",
    "  )\n",
    "\n",
    "\n",
    "class MixerBlock(nnx.Module):\n",
    "  def __init__(\n",
    "    self, num_patches, hidden_size, mix_patch_size, mix_hidden_size, *, rngs\n",
    "  ):\n",
    "    self.patch_mixer = MLP(num_patches, mix_patch_size, num_patches, rngs=rngs)\n",
    "    self.hidden_mixer = MLP(\n",
    "      hidden_size, mix_hidden_size, hidden_size, rngs=rngs\n",
    "    )\n",
    "    self.norm1 = nnx.LayerNorm(num_patches, rngs=rngs)\n",
    "    self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n",
    "\n",
    "  def __call__(self, y):\n",
    "    y = einop(y, '... p c -> ... c p')\n",
    "    y = y + self.patch_mixer(self.norm1(y))\n",
    "    y = einop(y, '... c p -> ... p c')\n",
    "    y = y + self.hidden_mixer(self.norm2(y))\n",
    "    return y\n",
    "\n",
    "\n",
    "class Mixer2d(nnx.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    hidden_size,\n",
    "    mix_patch_size,\n",
    "    mix_hidden_size,\n",
    "    num_blocks,\n",
    "    t1,\n",
    "    *,\n",
    "    rngs,\n",
    "  ):\n",
    "    height, width, input_size = img_size\n",
    "    assert (height % patch_size) == 0\n",
    "    assert (width % patch_size) == 0\n",
    "    num_patches = (height // patch_size) * (width // patch_size)\n",
    "\n",
    "    self.conv_in = nnx.Conv(\n",
    "      input_size + 1,\n",
    "      hidden_size,\n",
    "      kernel_size=(patch_size, patch_size),\n",
    "      strides=(patch_size, patch_size),\n",
    "      rngs=rngs,\n",
    "    )\n",
    "    self.conv_out = nnx.ConvTranspose(\n",
    "      hidden_size,\n",
    "      input_size,\n",
    "      kernel_size=(patch_size, patch_size),\n",
    "      strides=(patch_size, patch_size),\n",
    "      rngs=rngs,\n",
    "    )\n",
    "    self.blocks = [\n",
    "      MixerBlock(\n",
    "        num_patches, hidden_size, mix_patch_size, mix_hidden_size, rngs=rngs\n",
    "      )\n",
    "      for _ in range(num_blocks)\n",
    "    ]\n",
    "    self.norm = nnx.LayerNorm(\n",
    "      num_patches, reduction_axes=-2, feature_axes=-2, rngs=rngs\n",
    "    )\n",
    "    self.t1 = t1\n",
    "\n",
    "  def __call__(self, x, t):\n",
    "    t = t / self.t1\n",
    "    _, height, width, _ = x.shape\n",
    "    t = einop(t, 'b -> b h w 1', h=height, w=width)\n",
    "    x = jnp.concatenate([x, t], axis=-1)\n",
    "    x = self.conv_in(x)\n",
    "    _, patch_height, patch_width, _ = x.shape\n",
    "    x = einop(x, 'b h w c -> b (h w) c')\n",
    "    for block in self.blocks:\n",
    "      x = block(x)\n",
    "    x = self.norm(x)\n",
    "    x = einop(x, 'b (h w) c -> b h w c', h=patch_height, w=patch_width)\n",
    "    return self.conv_out(x)\n",
    "\n",
    "\n",
    "model = Mixer2d(\n",
    "  img_size=(128, 128, 3),\n",
    "  patch_size=8,\n",
    "  hidden_size=64,\n",
    "  mix_patch_size=256,\n",
    "  mix_hidden_size=256,\n",
    "  num_blocks=4,\n",
    "  t1=10.0,\n",
    "  rngs=rngs,\n",
    ")\n",
    "t_sample = jnp.full((x_sample.shape[0],), 100)\n",
    "y_sample = model(x_sample, t_sample)\n",
    "\n",
    "nnx.display(y_sample[0])\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generic, TypeVar\n",
    "\n",
    "M = TypeVar('M', bound=nnx.Module)\n",
    "\n",
    "\n",
    "class EMA(nnx.GraphNode, Generic[M]):\n",
    "  def __init__(self, model: M, decay: float):\n",
    "    self.decay = decay\n",
    "    self.model = model\n",
    "    self.graphdef, self.ema_params, self.rest = nnx.split(model, nnx.Param, ...)\n",
    "    # copy arrays to avoid aliasing\n",
    "    self.ema_params = jax.tree.map(jnp.array, self.ema_params)\n",
    "\n",
    "  def ema_model(self):\n",
    "    return nnx.merge(self.graphdef, self.ema_params, self.rest)\n",
    "\n",
    "  @nnx.jit\n",
    "  def update(self):\n",
    "    def _ema(ema_params, new_params):\n",
    "      return self.decay * ema_params + (1.0 - self.decay) * new_params\n",
    "\n",
    "    new_params = nnx.state(self.model, nnx.Param)\n",
    "    self.ema_params: nnx.State = jax.tree.map(_ema, self.ema_params, new_params)\n",
    "\n",
    "\n",
    "ema = EMA(model, config.ema_decay)\n",
    "\n",
    "nnx.display(ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "tx = optax.chain(\n",
    "  optax.clip_by_global_norm(1.0),\n",
    "  optax.adamw(\n",
    "    optax.piecewise_constant_schedule(\n",
    "      config.lr_start,\n",
    "      {\n",
    "        int(config.total_steps * 1 / 3): config.drop_1_mult,\n",
    "        int(config.total_steps * 2 / 3): config.drop_2_mult,\n",
    "      },\n",
    "    )\n",
    "  ),\n",
    ")\n",
    "optimizer = nnx.Optimizer(model, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "if config.loss_type == 'mse':\n",
    "  loss_metric = lambda a, b: jnp.mean((a - b) ** 2)\n",
    "elif config.loss_type == 'mae':\n",
    "  loss_metric = lambda a, b: jnp.mean(jnp.abs(a - b))\n",
    "else:\n",
    "  raise ValueError(f'Unknown loss type {config.loss_type}')\n",
    "\n",
    "\n",
    "def loss_fn(model: Mixer2d, xt, t, noise):\n",
    "  pred_noise = model(xt, t)\n",
    "  return loss_metric(noise, pred_noise)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(\n",
    "  model: Mixer2d,\n",
    "  ema: EMA[Mixer2d],\n",
    "  optimizer: nnx.Optimizer,\n",
    "  schedule: DDPMSchedule,\n",
    "  x: jax.Array,\n",
    "  rngs: nnx.Rngs,\n",
    "):\n",
    "  print(\"compiling 'train_step' ...\")\n",
    "  ema_model = ema.ema_model()\n",
    "  t = jax.random.randint(\n",
    "    rngs.schedule(),\n",
    "    x.shape[0:1],\n",
    "    minval=0,\n",
    "    maxval=config.timesteps,\n",
    "    dtype=jnp.uint32,\n",
    "  )\n",
    "  xt, noise = schedule.forward(x, t)\n",
    "\n",
    "  loss, grads = nnx.value_and_grad(loss_fn)(model, xt, t, noise)\n",
    "  ema_loss = loss_fn(ema_model, xt, t, noise)\n",
    "\n",
    "  optimizer.update(grads)\n",
    "  return {'loss': loss, 'ema_loss': ema_loss}\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "print(jax.devices())\n",
    "\n",
    "axs_diffusion = None\n",
    "axs_metrics = None\n",
    "ds_iterator = ds.as_numpy_iterator()\n",
    "logs = {}\n",
    "history = {}\n",
    "step = 0\n",
    "disp_diffusion = None\n",
    "disp_metrics = None\n",
    "\n",
    "# %%\n",
    "\n",
    "for step in tqdm(\n",
    "  range(step, config.total_steps), total=config.total_steps, unit='step'\n",
    "):\n",
    "  if step % config.eval_every == 0:\n",
    "    # --------------------\n",
    "    # visualize progress\n",
    "    # --------------------\n",
    "    n_rows = 3\n",
    "    n_cols = 5\n",
    "    viz_key = jax.random.PRNGKey(1)\n",
    "    x0 = jax.random.normal(viz_key, (n_rows * n_cols, *x_sample.shape[1:]))\n",
    "\n",
    "    ts = np.arange(config.timesteps)[::-1]\n",
    "    xf = sample(model, x0, ts, schedule, return_all=False)\n",
    "    xf = np.asarray(xf)\n",
    "    xf = einop(\n",
    "      xf, '(row col) h w c -> (row h) (col w) c', row=n_rows, col=n_cols\n",
    "    )\n",
    "\n",
    "    if axs_diffusion is None:\n",
    "      plt.figure(figsize=(3 * n_cols, 3 * n_rows))\n",
    "      disp_diffusion = display.display(\"diffusion\", display_id=True)\n",
    "      axs_diffusion = plt.gca()\n",
    "\n",
    "    # plt.figure(figsize=(3 * n_cols, 3 * n_rows))\n",
    "    # clear_output(wait=True)\n",
    "    axs_diffusion.clear()\n",
    "    xf = (xf + 1.0) / 2.0\n",
    "    xf = np.clip(xf, 0, 1)\n",
    "    axs_diffusion.imshow(xf)\n",
    "    disp_diffusion.update(axs_diffusion.figure)\n",
    "    # plt.show()\n",
    "    # plt.pause(0.1)\n",
    "  # --------------------\n",
    "  # trainig step\n",
    "  # --------------------\n",
    "  x = ds_iterator.next()\n",
    "  logs = train_step(model, ema, optimizer, schedule, x, rngs)\n",
    "  logs = jax.tree.map(np.asarray, logs)  # convert to numpy\n",
    "  for k, v in logs.items():\n",
    "    history.setdefault(k, []).append(v)\n",
    "\n",
    "  if (\n",
    "    step >= config.ema_update_after_step and step % config.ema_update_every == 0\n",
    "  ):\n",
    "    ema.update()\n",
    "\n",
    "  if step % config.log_every == 0:\n",
    "    if axs_metrics is None:\n",
    "      plt.figure(figsize=(12, 6))\n",
    "      disp_metrics = display.display('metrics', display_id=True)\n",
    "      axs_metrics = plt.gca()\n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    axs_metrics.clear()\n",
    "    for k, v in history.items():\n",
    "      axs_metrics.plot(v, label=k)\n",
    "\n",
    "    # force render\n",
    "    # axs_metrics.get_figure().show()\n",
    "    disp_metrics.update(axs_metrics.figure)\n",
    "\n",
    "# %%\n",
    "n_rows = 3\n",
    "n_cols = 5\n",
    "viz_key = jax.random.PRNGKey(1)\n",
    "t = jax.random.normal(viz_key, (n_rows * n_cols, *x_sample.shape[1:]))\n",
    "\n",
    "ts = np.arange(config.timesteps)[::-1]\n",
    "xf = sample(model, t, ts, schedule, return_all=False)\n",
    "xf = np.asarray(xf)\n",
    "xf = einop(xf, '(row col) h w c -> (row h) (col w) c', row=n_rows, col=n_cols)\n",
    "\n",
    "plt.figure(figsize=(3 * n_cols, 3 * n_rows))\n",
    "render_image(xf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 3\n",
    "n_cols = 5\n",
    "viz_key = jax.random.PRNGKey(1)\n",
    "x0 = jax.random.normal(viz_key, (n_rows * n_cols, *x_sample.shape[1:]))\n",
    "\n",
    "ts = np.arange(config.timesteps)[::-1]\n",
    "xf = sample(model, x0, ts, schedule, return_all=False)\n",
    "xf = np.asarray(xf)\n",
    "plt.imshow(xf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_data[1])\n",
    "plt.figure()\n",
    "plt.imshow(x_data_noise[1])\n",
    "plt.figure()\n",
    "plt.imshow(x_data[1] - x_data_noise[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
